(window.webpackJsonp=window.webpackJsonp||[]).push([[12],{366:function(t,s,n){"use strict";n.r(s);var a=n(45),e=Object(a.a)({},(function(){var t=this,s=t.$createElement,n=t._self._c||s;return n("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[n("h1",{attrs:{id:"eagerpy-types"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#eagerpy-types"}},[t._v("#")]),t._v(" eagerpy.types")]),t._v(" "),n("div",{staticClass:"language-py extra-class"},[n("pre",{pre:!0,attrs:{class:"language-py"}},[n("code",[n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" typing "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" Union"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" Tuple"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" TYPE_CHECKING\n\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" TYPE_CHECKING"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# for static analyzers")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" torch  "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# noqa: F401")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" tensorflow  "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# noqa: F401")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" jax  "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# noqa: F401")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" numpy  "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# noqa: F401")]),t._v("\n\nAxes "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Tuple"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("int")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\nAxisAxes "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Union"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("int")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" Axes"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\nShape "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Tuple"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("int")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\nShapeOrScalar "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Union"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("Shape"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("int")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# tensorflow.Tensor, jax.numpy.ndarray and numpy.ndarray currently evaluate to Any")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# we can therefore only provide additional type information for torch.Tensor")]),t._v("\nNativeTensor "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Union"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"torch.Tensor"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"tensorflow.Tensor"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"jax.numpy.ndarray"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"numpy.ndarray"')]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n")])])])])}),[],!1,null,null,null);s.default=e.exports}}]);